{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f71c9b35",
   "metadata": {},
   "source": [
    "# ASSIGNMENT 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6c9ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's Import required Libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#Let's Import selenium webdriver \n",
    "from selenium import webdriver\n",
    "\n",
    "#Let's Import required Exceptions which needs to handled\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException, ElementNotInteractableException\n",
    "\n",
    "#Let's Import requests\n",
    "import requests\n",
    "\n",
    "#Let's Importing regex\n",
    "import re\n",
    "\n",
    "#Let's Import support drivers\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "#Let's Import Time Out Exception\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from datetime import datetime\n",
    "\n",
    "#Let's Suppress warnings for a clean notebook\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced01a2f",
   "metadata": {},
   "source": [
    "Q1. Scrape the details of most viewed videos on YouTube from Wikipedia:Url = https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos/You need to find following details:\n",
    "A) Rank\n",
    "B) Name\n",
    "C) Artist\n",
    "D) Upload date\n",
    "E) Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49d6e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chrome browser\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2943883d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the specified url\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\"\n",
    "driver.get(url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7216ca2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating empty list\n",
    "Rank = []\n",
    "Name = []\n",
    "Artist = []\n",
    "Upload_date = []\n",
    "Views = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8086ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of rank details\n",
    "try:\n",
    "    for i in driver.find_elements_by_xpath(\"//table[@class='wikitable sortable jquery-tablesorter']//tbody//tr//td[1]\")[:30]:\n",
    "        Rank.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Rank.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7899d06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of video names\n",
    "try:\n",
    "    for i in driver.find_elements_by_xpath(\"//table[@class='wikitable sortable jquery-tablesorter']//tbody//tr//td[2]\")[:30]:\n",
    "        Name.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Name.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b52e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of artist name\n",
    "try:\n",
    "    for i in driver.find_elements_by_xpath(\"//table[@class='wikitable sortable jquery-tablesorter']//tbody//tr//td[3]\")[:30]:\n",
    "        Artist.append(i.text)\n",
    "except NoSuchElementException:        \n",
    "    Artist.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2c15ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of upload dates\n",
    "try:\n",
    "    for i in driver.find_elements_by_xpath(\"//table[@class='wikitable sortable jquery-tablesorter']//tbody//tr//td[5]\")[:30]:\n",
    "        Upload_date.append(i.text)\n",
    "except NoSuchElementException:        \n",
    "    Upload_date.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548c29bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of views\n",
    "try:\n",
    "    for i in driver.find_elements_by_xpath(\"//table[@class='wikitable sortable jquery-tablesorter']//tbody//tr//td[4]\")[:30]:\n",
    "        Views.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Views.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf81d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA FRAMEING\n",
    "Youtube=pd.DataFrame({})\n",
    "Youtube['Rank'] = Rank\n",
    "Youtube['Name'] = Name\n",
    "Youtube['Artist'] = Artist\n",
    "Youtube['Uploaded Date'] = Upload_date\n",
    "Youtube['Views'] = Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f574eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing data frame\n",
    "Youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5a4097",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d62555f",
   "metadata": {},
   "source": [
    "Q2. Scrape the details team Indiaâ€™s international fixtures from bcci.tv. Url = https://www.bcci.tv/.\n",
    "You need to find following details:\n",
    "A) Match title (I.e. 1st ODI)\n",
    "B) Series\n",
    "C) Place\n",
    "D) Date\n",
    "E) Time\n",
    "Note: - From bcci.tv home page you have reach to the international fixture page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfe430d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activating the chrome browser\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b64991",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chrome browser\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "\n",
    "#Getting URL\n",
    "url='https://www.bcci.tv/'\n",
    "driver.get(url)\n",
    "\n",
    "\n",
    "driver.maximize_window()\n",
    "\n",
    "#Getting International Fixture\n",
    "International=driver.find_element_by_xpath('/html/body/nav/div/div[2]/ul[1]/li[2]/a')\n",
    "International.click()\n",
    "\n",
    "#Getting Fixtures\n",
    "fixtures=driver.find_element_by_xpath('/html/body/div[3]/div[1]/div/ul/li[1]/a')\n",
    "fixtures.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ede0f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_title=[]\n",
    "try:\n",
    "    title=driver.find_elements_by_xpath('//span[@class=\"ng-binding\"]')\n",
    "    for i in title:\n",
    "        match_title.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    match_title.append('-----')\n",
    "match_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a043ff55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetching url for each product\n",
    "prod_url = []\n",
    "url = driver.find_elements_by_xpath('//a[@class=\"match-center-btn ng-scope\"]')\n",
    "for i in url:\n",
    "    prod_url.append(i.get_attribute('href'))\n",
    "prod_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d9827b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Place=[]\n",
    "try:\n",
    "    place=driver.find_elements_by_xpath('//span[@class=\"ng-binding ng-scope\"]')\n",
    "    for i in place:\n",
    "        Place.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Place.append('-')\n",
    "Place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374dac56",
   "metadata": {},
   "outputs": [],
   "source": [
    "Series=[]\n",
    "try:\n",
    "    series=driver.find_elements_by_xpath('//span[@class=\"matchOrderText ng-binding ng-scope\"]')\n",
    "    for i in series:\n",
    "        Series.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Searies.append('----')\n",
    "Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa266e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Date=[]\n",
    "try:\n",
    "    date=driver.find_elements_by_xpath('//h5[@class=\"ng-binding\"]')\n",
    "    for i in date:\n",
    "        Date.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Date.append('-')\n",
    "Date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245f02fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Time=[]\n",
    "try:\n",
    "    time=driver.find_elements_by_xpath('//h5[@class=\"text-right ng-binding\"]')\n",
    "    for i in time:\n",
    "        Time.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Time.append('-')\n",
    "Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bd95af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting all in a DataFrame\n",
    "Indias_international_fixtures=pd.DataFrame({})\n",
    "Indias_international_fixtures['Match Title']=match_title\n",
    "Indias_international_fixtures['Place']=Place\n",
    "Indias_international_fixtures['Date']=Date\n",
    "Indias_international_fixtures['Time']=Time\n",
    "Indias_international_fixtures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ebf0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270805f6",
   "metadata": {},
   "source": [
    "3. Scrape the details of selenium exception from guru99.com. Url = https://www.guru99.com/ You need to find following details:\n",
    "A) Name\n",
    "B) Description\n",
    "Note: - From guru99 home page you have to reach to selenium exception handling page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0d5b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activating the chrome browser\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "\n",
    "#getting the specified url\n",
    "url = \"https://www.guru99.com/\"\n",
    "driver.get(url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a426b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Going to search bar and searching for selenium exception\n",
    "ser_bar = driver.find_element_by_xpath(\"/html/body/div[1]/div/div/div/main/div/article/div/div[1]/div[1]/div[2]/div/div[1]/div/div/div/form/table/tbody/tr/td[1]/div/table/tbody/tr/td[1]/input\")\n",
    "ser_bar.send_keys(\"selenium exception handling\")\n",
    "ser_but = driver.find_element_by_xpath(\"/html/body/div[1]/div/div/div/main/div/article/div/div[1]/div[1]/div[2]/div/div[1]/div/div/div/form/table/tbody/tr/td[2]/button\").click()\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51ec96f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e58d7db",
   "metadata": {},
   "source": [
    "4. Scrape the details of State-wise GDP of India from statisticstime.com.\n",
    "Url = http://statisticstimes.com/\n",
    "You have to find following details:\n",
    "A) Rank\n",
    "B) State\n",
    "C) GSDP\n",
    "D) GSDP\n",
    "E) Share\n",
    "F) GDP($ billion)\n",
    "Note: - From statisticstimes home page you have to reach to economy page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58622bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activating the chrome browser\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "\n",
    "# Let's open the URL\n",
    "driver.get('http://statisticstimes.com/')\n",
    "driver.implicitly_wait(3)\n",
    "\n",
    "# Let's click on the economy and then on India from there\n",
    "Economy_India=driver.find_element_by_xpath(\"//div[@class='navbar']/div[2]/div/a[3]\")\n",
    "driver.get(Economy_India.get_attribute('href')) \n",
    "\n",
    "# Let's click on the GDP of Indian states \n",
    "GDP_Indian_States=driver.find_element_by_xpath(\"//div[@style='float:left;background-color:seashell;width:400px;height:800px;']/ul/li[1]/a\")\n",
    "driver.get(GDP_Indian_States.get_attribute('href'))\n",
    "\n",
    "# Let's create an empty lists to store the scraping data\n",
    "Rank=[] \n",
    "State=[] \n",
    "GSDP_Year_2019_to_Year_2020=[] \n",
    "GSDP_Year_2018_to_Year_2019=[] \n",
    "Share_Year_2018_to_2019=[] \n",
    "GDP_Dollar_Billion=[]\n",
    "\n",
    "# Let's create a function\n",
    "rank = driver.find_elements_by_xpath('//table[@id=\"table_id\"]/tbody/tr/td[1]')\n",
    "for i in rank:\n",
    "    Rank.append(i.text)\n",
    "state = driver.find_elements_by_xpath('//table[@id=\"table_id\"]/tbody/tr/td[2]')\n",
    "for i in state:\n",
    "    State.append(i.text)\n",
    "gdsp_19_20 = driver.find_elements_by_xpath('//table[@id=\"table_id\"]/tbody/tr/td[3]')\n",
    "for i in gdsp_19_20:\n",
    "    GSDP_Year_2019_to_Year_2020.append(i.text)\n",
    "gdsp_18_19 = driver.find_elements_by_xpath('//table[@id=\"table_id\"]/tbody/tr/td[4]')\n",
    "for i in gdsp_18_19:\n",
    "    GSDP_Year_2018_to_Year_2019.append(i.text)\n",
    "share = driver.find_elements_by_xpath('//table[@id=\"table_id\"]/tbody/tr/td[5]')\n",
    "for i in share:\n",
    "    Share_Year_2018_to_2019.append(i.text)\n",
    "gdp_dollar = driver.find_elements_by_xpath('//table[@id=\"table_id\"]/tbody/tr/td[6]')\n",
    "for i in gdp_dollar:\n",
    "    GDP_Dollar_Billion.append(i.text)\n",
    "    \n",
    "# Let's create the dataframe from the scraped data\n",
    "GSDP=pd.DataFrame({})\n",
    "GSDP['Rank']=Rank \n",
    "GSDP['State']=State\n",
    "GSDP['GSDP (Cr INR at Current prices) Year 2018-2019']=GSDP_Year_2018_to_Year_2019 \n",
    "GSDP['GSDP (Cr INR at Current prices) Year 2019-2020']=GSDP_Year_2019_to_Year_2020 \n",
    "GSDP['Share Year 2018-2019']=Share_Year_2018_to_2019 \n",
    "GSDP['GDP ($ Billion)']=GDP_Dollar_Billion\n",
    "GSDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718f80fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb54eba7",
   "metadata": {},
   "source": [
    "Q5 Scrape the details of trending repositories on Github.com.\n",
    "Url = https://github.com/\n",
    "You have to find the following details:\n",
    "A) Repository title\n",
    "B) Repository description\n",
    "C) Contributors count\n",
    "D) Language used\n",
    "ASSIGNMENT\n",
    "Note: - From the home page you have to click on the trending option from Explore menu through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a351ea3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029fe714",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chrome browser\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "\n",
    "#getting the specified url\n",
    "url = \"https://github.com/\"\n",
    "driver.get(url)\n",
    "time.sleep(3)\n",
    "\n",
    "#Getting explore button and clicking on it\n",
    "explore = driver.find_element_by_xpath(\"/html/body/div[1]/header/div/div[2]/nav/ul/li[4]/details\").click()\n",
    "\n",
    "#Selecting trending option\n",
    "trend_url = driver.find_element_by_xpath(\"/html/body/div[1]/header/div/div[2]/nav/ul/li[4]/details/div/ul/li[5]/a\")\n",
    "urls = trend_url.get_attribute(\"href\")\n",
    "driver.get(urls)\n",
    "\n",
    "#Creating empty list\n",
    "repo_urls = []\n",
    "rep_title = []\n",
    "Description = []\n",
    "Contributors = []\n",
    "Language = []\n",
    "lang = []\n",
    "\n",
    "#Fetching urls for each repository\n",
    "repository = driver.find_elements_by_xpath(\"//h1[@class = 'h3 lh-condensed']//a\")\n",
    "for i in repository:\n",
    "    repo_urls.append(i.get_attribute(\"href\"))\n",
    "\n",
    "    \n",
    "#Scraping data of repository title\n",
    "title = driver.find_elements_by_xpath(\"//h1[@class = 'h3 lh-condensed']\")\n",
    "for i in title:\n",
    "    rep_title.append(i.text)\n",
    "\n",
    "    \n",
    "#Scraping data from every repository page\n",
    "for i in repo_urls:\n",
    "    driver.get(i)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    \n",
    "    #Scraping data of decription\n",
    "    try:\n",
    "        desc = driver.find_element_by_xpath(\"//p[@class='f4 mt-3']\")\n",
    "        Description.append(desc.text)\n",
    "    except NoSuchElementException:\n",
    "        Description.append('-')\n",
    "        \n",
    "        \n",
    "    #Scraping data of contributors\n",
    "    try:\n",
    "        cont_tags = driver.find_element_by_xpath(\"//*[contains(text(),'    Contributors ')]\")\n",
    "        Contributors.append(cont_tags.text.replace('Contributors',''))\n",
    "    except NoSuchElementException:\n",
    "        Contributors.append('-')\n",
    "        \n",
    "        \n",
    "    #fetching Languages used\n",
    "    try:\n",
    "        for i in driver.find_elements_by_xpath(\"//span[@class='color-text-primary text-bold mr-1']\"):\n",
    "            lang.append(i.text)\n",
    "        Language.append(lang)\n",
    "    except NoSuchElementException:\n",
    "        Language.append('-')\n",
    "        \n",
    "#DATA FRAMEING\n",
    "\n",
    "Github=pd.DataFrame({})\n",
    "Github['Repository title'] = rep_title\n",
    "Github['Repository description'] = Description\n",
    "Github['Contributors count'] = Contributors\n",
    "Github['Language used'] = Language\n",
    "\n",
    "\n",
    "#Printing data frame\n",
    "Github\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f22900",
   "metadata": {},
   "outputs": [],
   "source": [
    "#closing web driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27ba192",
   "metadata": {},
   "source": [
    "Q6 Scrape the details of top 100 songs on billiboard.com.\n",
    "Url = https:/www.billboard.com/\n",
    "You have to find the following details:\n",
    "A) Song name\n",
    "B) Artist name\n",
    "C) Last week rank\n",
    "D) Peak rank\n",
    "E) Weeks on board\n",
    "Note: - From the home page you have to click on the charts option then hot 100-page link through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07eb3781",
   "metadata": {},
   "outputs": [],
   "source": [
    "#opening web driver\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "url = \"https://www.billboard.com/\"\n",
    "driver.get(url)\n",
    "driver.maximize_window()\n",
    "\n",
    "#going to specified url by code\n",
    "charts= driver.find_element_by_xpath(\"/html/body/div[4]/header/div[1]/div/div/div[2]/div/nav/ul/li[1]/a\")\n",
    "charts.click()\n",
    "view = driver.find_element_by_xpath(\"/html/body/div[4]/main/div[2]/div[1]/div[1]/div/div/div[3]/a\")\n",
    "view.click()\n",
    "\n",
    "#creating empty list\n",
    "song = []\n",
    "artist = []\n",
    "last = []\n",
    "peak = []\n",
    "weeks = []\n",
    "\n",
    "#Scraping data\n",
    "\n",
    "for i in driver.find_elements_by_xpath('//div[@class=\"o-chart-results-list-row-container\"]//ul//li//ul//li[1]/h3'):\n",
    "    song.append(i.text)\n",
    "    \n",
    "for i in driver.find_elements_by_xpath('//div[@class=\"o-chart-results-list-row-container\"]//ul//li//ul//li[1]/span'):\n",
    "    artist.append(i.text)\n",
    "    \n",
    "for i in driver.find_elements_by_xpath('//div[@class=\"o-chart-results-list-row-container\"]//ul//li//ul//li[4]/span'):\n",
    "    last.append(i.text)\n",
    "    \n",
    "for i in driver.find_elements_by_xpath('//div[@class=\"o-chart-results-list-row-container\"]//ul//li//ul//li[5]/span'):\n",
    "    peak.append(i.text)\n",
    "    \n",
    "for i in driver.find_elements_by_xpath('//div[@class=\"o-chart-results-list-row-container\"]//ul//li//ul//li[6]/span'):\n",
    "    weeks.append(i.text)\n",
    "print(len(artist),len(song),len(last),len(peak),len(weeks))\n",
    "\n",
    "#creating Data frame\n",
    "billboard = pd.DataFrame({'Song Name':song,'Artist Name':artist,'Last Week Rank':last[:100],'Peak Rank':peak[:100],'Weeks on Board':weeks})\n",
    "billboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad9cc08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d1f892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f17c13e0",
   "metadata": {},
   "source": [
    "7. Scrape the details of Data science recruiters from naukri.com.\n",
    "Url = https://www.naukri.com/\n",
    "You have to find the following details:\n",
    "A) Name\n",
    "B) Designation\n",
    "C) Company\n",
    "D) Skills they hire for\n",
    "E) Location\n",
    "Note: - From naukri.com homepage click on the recruiters option and the on the search pane type Data science and\n",
    "click on search. All this should be done through code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e312f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activating the chrome browser\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "\n",
    "#getting the specified url\n",
    "url = \"https://www.naukri.com/\"\n",
    "driver.get(url)\n",
    "time.sleep(3)\n",
    "\n",
    "#Fetching urls for recruiter page\n",
    "job = driver.find_element_by_xpath('/html/body/div/div[2]/div[1]/div/ul/li[1]/a').click()\n",
    "\n",
    "rec = driver.find_element_by_xpath('/html/body/div[1]/div/ul/li[2]/a')\n",
    "page_url = rec.get_attribute(\"href\")\n",
    "\n",
    "driver.get(page_url)\n",
    "time.sleep(3)\n",
    "\n",
    "#Fetching search button,bar xpath and clicking on it\n",
    "search = driver.find_element_by_xpath(\"//div[@class='inpWrap']//input\") \n",
    "search.send_keys(\"Data science \")           \n",
    "but = driver.find_element_by_xpath(\"//button[@class='fl qsbSrch blueBtn']\").click()     \n",
    "time.sleep(4)\n",
    "#Creating empty list\n",
    "Name = []\n",
    "Designation = []\n",
    "Company = []\n",
    "Skills = []\n",
    "Location = []\n",
    "#Scraping data of Names\n",
    "for i in driver.find_elements_by_xpath(\"//span[@class='fl ellipsis']\")[:50]:\n",
    "    Name.append(i.text)\n",
    "time.sleep(5)\n",
    "\n",
    "\n",
    "#Scraping data of Designation\n",
    "for i in driver.find_elements_by_xpath(\"//span[@class='ellipsis clr']\")[:50]:\n",
    "    Designation.append(i.text)\n",
    "time.sleep(5)\n",
    "\n",
    "#Scraping data of company name\n",
    "for i in driver.find_elements_by_xpath(\"//div[@class='vcard']//p[1]/a[2]\")[:50]:\n",
    "    Company.append(i.text)\n",
    "time.sleep(5)\n",
    "    \n",
    "#scraping data of Skills \n",
    "for i in driver.find_elements_by_xpath(\"//div[@class='hireSec highlightable']\")[:50]:\n",
    "    try:\n",
    "        if i.text == \"Not Specified\": raise NoSuchElementException\n",
    "        Skills.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        Skills.append('-')\n",
    "    time.sleep(5)\n",
    "\n",
    "#Scraping data of locations\n",
    "for i in driver.find_elements_by_xpath(\"//div[@class='vcard']//p[1]//span\")[:50]:\n",
    "    Location.append(i.text)\n",
    "time.sleep(5)\n",
    "#DATA FRAMEING\n",
    "Naukri=pd.DataFrame({})\n",
    "Naukri['Name'] = Name\n",
    "Naukri['Designation'] = Designation\n",
    "Naukri['Company'] = Company\n",
    "Naukri['Skills'] = Skills\n",
    "Naukri['Location'] = Location\n",
    "#Printing data frame\n",
    "Naukri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b69fd35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa3fd15b",
   "metadata": {},
   "source": [
    "8. Scrape the details of Highest selling novels.\n",
    "Url = https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-greycompare/\n",
    "You have to find the following details:\n",
    "A) Book name\n",
    "B) Author name\n",
    "C) Volumes sold\n",
    "D) Publisher\n",
    "E) Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c720d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chrome browser\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab28d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the specified url\n",
    "url = \"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare/\"\n",
    "driver.get(url)\n",
    "time.sleep(3)\n",
    "#Creating empty list\n",
    "Book_name = []\n",
    "Author = []\n",
    "Volumes_sold = []\n",
    "Publisher = []\n",
    "Genre = []\n",
    "#scraping data of book names\n",
    "for i in driver.find_elements_by_xpath(\"//tbody//tr/td[2]\"):\n",
    "    Book_name.append(i.text)\n",
    "\n",
    "    \n",
    "#Scraping data of author names\n",
    "for i in driver.find_elements_by_xpath(\"//tbody/tr/td[3]\"):\n",
    "    try:\n",
    "        if i.text == '0' : raise NoSuchElementException\n",
    "        Author.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        Author.append('-')\n",
    "time.sleep(1)\n",
    "\n",
    "\n",
    "#Scraping data of volumes sold\n",
    "for i in driver.find_elements_by_xpath(\"//tbody/tr/td[4]\"):\n",
    "    Volumes_sold.append(i.text)\n",
    "\n",
    "    \n",
    "#Scraping data of publisher names\n",
    "for i in driver.find_elements_by_xpath(\"//tbody/tr/td[5]\"):\n",
    "    Publisher.append(i.text)\n",
    "\n",
    "    \n",
    "#Scraping data of genre\n",
    "for i in driver.find_elements_by_xpath(\"//tbody/tr/td[6]\"):\n",
    "    Genre.append(i.text)\n",
    "#DATA FRAMEING\n",
    "Novel=pd.DataFrame({})\n",
    "Novel['Book Name'] = Book_name\n",
    "Novel['Author'] = Author\n",
    "Novel['Volume sold'] = Volumes_sold\n",
    "Novel['Publisher'] = Publisher\n",
    "Novel['Genre'] = Genre\n",
    "#Printing data frame\n",
    "Novel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adc034b",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1763fe",
   "metadata": {},
   "source": [
    "9. Scrape the details most watched tv series of all time from imdb.com.\n",
    "Url = https://www.imdb.com/list/ls095964455/\n",
    "You have to find the following details:\n",
    "A) Name\n",
    "B) Year span\n",
    "C) Genre\n",
    "D) Run time\n",
    "E) Ratings\n",
    "F) Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc54fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activating the chrome browser\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "\n",
    "#getting the specified url\n",
    "url = \"https://www.imdb.com/list/ls095964455/\"\n",
    "driver.get(url)\n",
    "time.sleep(3)\n",
    "\n",
    "#Creating empty lists.\n",
    "Name = []\n",
    "Year_span = []\n",
    "Genre = []\n",
    "Run_time = []\n",
    "Ratings = []\n",
    "Votes = []\n",
    "\n",
    "#Scraping data of Names\n",
    "for i in driver.find_elements_by_xpath(\"//h3[@class='lister-item-header']/a\"):\n",
    "    Name.append(i.text)\n",
    "\n",
    "    \n",
    "#Scraping data of Year span\n",
    "for i in driver.find_elements_by_xpath(\"//span[@class='lister-item-year text-muted unbold']\"):\n",
    "    Year_span.append(i.text)\n",
    "\n",
    "    \n",
    "#Scraping data of genre\n",
    "for i in driver.find_elements_by_xpath(\"//span[@class='genre']\"):\n",
    "    Genre.append(i.text)\n",
    "\n",
    "    \n",
    "#Scraping data of Run time\n",
    "for i in driver.find_elements_by_xpath(\"//span[@class='runtime']\"):\n",
    "    Run_time.append(i.text)\n",
    "\n",
    "    \n",
    "#Scraping data of Ratings\n",
    "for i in driver.find_elements_by_xpath(\"//div[@class='ipl-rating-star small']//span[2]\"):\n",
    "    Ratings.append(i.text)\n",
    "\n",
    "    \n",
    "#Scraping data of votes\n",
    "for i in driver.find_elements_by_xpath(\"//div[@class='lister-item-content']//p[4]/span[2]\"):\n",
    "    Votes.append(i.text)\n",
    "#DATA FRAMEING\n",
    "IMDB=pd.DataFrame({})\n",
    "IMDB['Name'] = Name\n",
    "IMDB['Year Span'] = Year_span\n",
    "IMDB['Genre'] = Genre\n",
    "IMDB['Run Time'] = Run_time\n",
    "IMDB['Ratings'] = Ratings\n",
    "IMDB['Votes'] = Votes\n",
    "#Printing data frame\n",
    "IMDB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8396e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079f6eee",
   "metadata": {},
   "source": [
    "10. Details of Datasets from UCI machine learning repositories.\n",
    "Url = https://archive.ics.uci.edu/\n",
    "You have to find the following details:\n",
    "A) Dataset name\n",
    "B) Data type\n",
    "C) Task\n",
    "D) Attribute type\n",
    "E) No of instances\n",
    "F) No of attribute\n",
    "G) Year\n",
    "Note: - from the home page you have to go to the ShowAllDataset page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a362670",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3045b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activating the chrome browser\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "\n",
    "#getting the specified url\n",
    "url = \"https://archive.ics.uci.edu/\"\n",
    "driver.get(url)\n",
    "time.sleep(3)\n",
    "\n",
    "#Finding view all dataset button\n",
    "view_dataset = driver.find_element_by_xpath(\"//tbody[1]//tr/td[2]/span[2]/a\")    \n",
    "page_url = view_dataset.get_attribute(\"href\")\n",
    "driver.get(page_url)\n",
    "time.sleep(5)\n",
    "\n",
    "#Getting page urls containing list of all datasets\n",
    "all_lst = driver.find_element_by_xpath(\"/html/body/table[2]/tbody/tr/td[2]/table[1]/tbody/tr/td[2]/p/a\")  \n",
    "lst_url = all_lst.get_attribute(\"href\")           \n",
    "driver.get(lst_url)\n",
    "time.sleep(5)\n",
    "\n",
    "#fetching url for each dataset\n",
    "data_url = driver.find_elements_by_xpath(\"//p[@class='normal']//b/a\")    \n",
    "\n",
    "urls = []     \n",
    "for i in data_url:\n",
    "    urls.append(i.get_attribute(\"href\"))\n",
    "#Creating empty list\n",
    "Data_name = []\n",
    "Data_type = []\n",
    "Task = []\n",
    "Attr_type = []\n",
    "Instances = []\n",
    "n_attributes = []\n",
    "Year = []\n",
    "#Scraping requiredinfo from urls\n",
    "for i in urls:\n",
    "    driver.get(i)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    \n",
    "    #Scraping Dataset name\n",
    "    try: \n",
    "        ds_name = driver.find_element_by_xpath(\"//span[@class='heading']\")\n",
    "        Data_name.append(ds_name.text)\n",
    "    except NoSuchElementException:\n",
    "        Data_name.append('-')\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    #Scraping data type\n",
    "    try:\n",
    "        dtype = driver.find_element_by_xpath(\"//table[@border='1']//tbody/tr/td[2]\")\n",
    "        if dtype.text == \"N/A\": raise NoSuchElementException\n",
    "        Data_type.append(dtype.text)\n",
    "    except NoSuchElementException:\n",
    "        Data_type.append('-')\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    #Scraping Attribute type\n",
    "    try:\n",
    "        atype = driver.find_element_by_xpath(\"//table[@border='1']//tbody/tr[2]/td[2]\")\n",
    "        if atype.text == \"N/A\": raise NoSuchElementException\n",
    "        Attr_type.append(atype.text)\n",
    "    except NoSuchElementException:\n",
    "        Attr_type.append('-')\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    #scraping Task\n",
    "    try:\n",
    "        task = driver.find_element_by_xpath(\"//table[@border='1']//tbody/tr[3]/td[2]\")\n",
    "        if task.text == \"N/A\": raise NoSuchElementException\n",
    "        Task.append(task.text)\n",
    "    except NoSuchElementException:\n",
    "        Task.append('-')\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    #Scraping No of instances\n",
    "    try:\n",
    "        inst = driver.find_element_by_xpath(\"//table[@border='1']//tbody/tr/td[4]\")\n",
    "        if inst.text == \"N/A\": raise NoSuchElementException\n",
    "        Instances.append(inst.text)\n",
    "    except NoSuchElementException:\n",
    "        Instances.append('-')\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    #Scraping  No of attribute\n",
    "    try:\n",
    "        attr = driver.find_element_by_xpath(\"//table[@border='1']//tbody/tr[2]/td[4]\")\n",
    "        if attr.text == \"N/A\": raise NoSuchElementException\n",
    "        n_attributes.append(attr.text)\n",
    "    except NoSuchElementException:\n",
    "        n_attributes.append('-')\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    #scraping year\n",
    "    try:\n",
    "        year = driver.find_element_by_xpath(\"//table[@border='1']//tbody/tr[2]/td[6]\")\n",
    "        if year.text == \"N/A\": raise NoSuchElementException\n",
    "        Year.append(year.text[:4])\n",
    "    except NoSuchElementException:\n",
    "        Year.append('-')\n",
    "    time.sleep(1)\n",
    "#DATA FRAMEING\n",
    "UCI=pd.DataFrame({})\n",
    "UCI['Data Name'] = Data_name\n",
    "UCI['Data Type'] = Data_type\n",
    "UCI['Task'] = Task\n",
    "UCI['Attribute type'] = Attr_type\n",
    "UCI['No of instance'] = Instances\n",
    "UCI['No of attributes'] = n_attributes\n",
    "UCI['Year'] = Year\n",
    "#Printing dataframe\n",
    "UCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6694132f",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e04af4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
